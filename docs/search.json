[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Working with Label Errors in Natural Language Datasets",
    "section": "",
    "text": "* Equal contribution.\nIn our paper, “Detecting Label Errors by using Pre-Trained Language Models”, to be published at the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022), we present a simple method for detecting label errors that can be used to improve performance in many natural language tasks. We also show that label errors have a much greater impact on model performance than previously believed.\nIn this blog post, we share a practical summary of our key findings about label errors that may be helpful when working with noisy natural language datasets. We then discuss more general implications for research into LNL (Learning with Noisy Labels)."
  },
  {
    "objectID": "blog.html#label-errors-can-be-detected-using-pre-trained-language-model-loss",
    "href": "blog.html#label-errors-can-be-detected-using-pre-trained-language-model-loss",
    "title": "Working with Label Errors in Natural Language Datasets",
    "section": "Label errors can be detected using pre-trained language model loss",
    "text": "Label errors can be detected using pre-trained language model loss\nIt is known empirically that data points with high loss often have unusual characteristics. We demonstrate that the loss of a fine-tuned pre-trained language model is strongly associated with likelihood of an out-of-sample data point being a label error, across a range of pre-trained language models and NLP benchmark datasets. “Out-of-sample” refers to data points which were not in the current training split. However, this does not prevent this technique from being used on training data, which can simply be divided into folds and cross-validated.\n\n\n\nFigure 1: Precision-recall curves for label error detection.\n\n\nSimply evaluating a small percentage of items in descending order of loss identifies a large proportion of label errors with high precision, and more effectively than both non-pre-trained baselines, and a more complex state-of-the-art error detection framework.\nTakeaway: Checking high loss data points can be useful as a quick and easy data quality health check, or for more rigorous data cleansing."
  },
  {
    "objectID": "blog.html#models-are-far-less-robust-to-label-errors-than-previously-believed",
    "href": "blog.html#models-are-far-less-robust-to-label-errors-than-previously-believed",
    "title": "Working with Label Errors in Natural Language Datasets",
    "section": "Models are far less robust to label errors than previously believed",
    "text": "Models are far less robust to label errors than previously believed\nDeep learning is thought to be robust to massive amounts of label noise. Models have been shown to achieve high accuracy in datasets with as many as 100 noisy items for each clean item (Rolnick et al. 2017), and research into learning with noisy labels focuses on label noise in the range of 20-80%. However, the majority of studies only use simple artificial noise, in which labels are randomly flipped with no regard to the input text.\nWe revisit the question of model robustness using a new form of realistic, human-originated label noise, which takes advantage of latent human error that exists within crowdsourced datasets.\n\n\n\nFigure 2: Human-originated label errors have relationships with input text. On this part of speech task example from TweetNLP, two crowd annotators have confused “advice” (noun) with “advise” (verb). Human-originated noise would simulate a label error by applying the label verb instead of the label noun, whereas existing methods for simulating noise may select an unrealistic error such as adjective.\n\n\nIn contrast to previous findings, we show that performance very quickly decreases as the amount of label noise increases. Models learn to reproduce patterns of human error from training data with as little as 5-15% label noise.\n\n\n\n\nFigure 3: Models are robust to simple uniform or class-based label noise, but not more realistic forms of noise.\n\n\n\nWe developed three noising protocols, which each simulate a different class of label error. The protocols are described in detail in our paper. For the most challenging class of noise, Crowd Majority, performance degradation was roughly linear with the amount of noise applied.\nTakeaway: Significant performance improvements can be achieved by cleaning noisy training datasets."
  },
  {
    "objectID": "blog.html#sec-eval",
    "href": "blog.html#sec-eval",
    "title": "Working with Label Errors in Natural Language Datasets",
    "section": "Validation label errors can harm performance more than training label errors",
    "text": "Validation label errors can harm performance more than training label errors\nThe majority of Learning with Noisy Labels research focuses on noise in training splits, and ignores noise in evaluation splits. The existence of a clean test split is usually assumed in order to fairly evaluate techniques for learning on noisy training data. However, recent work by Northcutt, Athalye, and Mueller (2021) calls attention to this gap, finding that the test splits of many popular ML benchmark datasets contain label errors, and that these errors destabilize benchmark performance.\nWe show that noise in test and validation splits has several harmful effects on model performance.\n\nTest split errors reduce and distort measurable performance\nTest splits are used to evaluate models’ true performance. These results factor into decisions about whether a model may be deployed, for example due to regulatory compliance requirements, or the needs of downstream users. But because real-world datasets have noisy test splits, measurements may not accurately reflect the true performance of a model.\nWe show that label errors in test splits generally results in measuring lower performance than the model would achieve in the real world. However, more challenging and realistic label errors can also cause the opposite effect. Models can learn erroneous features from errors in training data, and at higher levels of noise, this may enable “correct” predictions on erroneous test data to dominate and result in unfairly high performance, such as in Crowd Majority below.\n\n\n\n\nFigure 4: Noise in test splits generally reduces measurable performance, but has more complex effects for higher levels of more realistic noise.\n\n\n\nWe also show that using pre-trained language models to highlight likely errors for re-evaluation and cleaning moves measurable performance towards the true performance of the model.\nTakeaway: Given the prevalence of label errors in real-world datasets, typical error rates, and typical data cleaning effectiveness, we estimate that cleaning a small percentage of test split items can increase measurable performance by 1-2% in a large number of real-world NLP applications.\n\n\nValidation split errors cause poor model selection\nValidation splits are used to select the best model from a set of candidate models, such as might be produced by a hyperparameter search process. Our experiments show that label errors in validation splits can damage final performance by selecting of a model that performs best on noisy validation data, but not on real data. Correcting validation split errors generally selects a slightly better model, which results in a small improvement in final test performance, but does not fully correct the problem.\n\n\n\nTable 2: End-to-end effects of label noise on task performance, as evaluated on noisy, corrected, and clean validation and test data splits. True accuracy is measured on clean test sets, and measurable accuracy on noisy or corrected test sets. Rank is a relative measure of true accuracy; lower numerical ranks have higher accuracy. Corrections which improve or reduce performance metrics are highlighted in green or red, respectively. Metrics are evaluated on models trained on noisy data.\n\n\n\n\n\n\n\n\n\n\nEval.\nTest Perf.\nI-5\nA-5\nT-5\nT-M\n\n\n\n\nNoisy\nMeasurable\n90.1\n88.3\n89.3\n89.3\n\n\nNoisy\nTrue\n94.2\n91.0\n92.8\n82.0\n\n\nNoisy\nRank\n10\n1\n3\n10\n\n\nCorr.\nMeasurable\n95.1\n90.7\n92.9\n88.5\n\n\nCorr.\nTrue\n95.1\n90.8\n93.0\n82.0\n\n\nCorr.\nRank\n4\n5\n2\n8\n\n\nClean\nTrue\n95.8\n91.0\n93.8\n82.1\n\n\n\n\n\nTakeaway: Cleaning the validation split can slightly improve performance via better model selection."
  },
  {
    "objectID": "blog.html#sec-lnl",
    "href": "blog.html#sec-lnl",
    "title": "Working with Label Errors in Natural Language Datasets",
    "section": "New challenges in LNL",
    "text": "New challenges in LNL\nMost research into learning with noisy labels is conducted using simple artificial noise, as few datasets exist which contain real and known label errors. But simple artificial noise is no longer challenging; recent LNL analyses study conditions where up to 80% of labels are noised (Song et al. 2022). New capabilities provided by the advent of modern deep learning allows us to attack more challenging problems in LNL.\n\nArtificial noise behaves very differently to real and human-originated noise\nWe find that the characteristics of artificial noise are very different to those of real noise, as verified using annotators on Mechanical Turk. Simultaneously, we show that human-originated noise is much more similar to real noise.\n[Two loss diagrams go here]\nFundamentally, loss is high and models are robust because simple artificial noise permutes labels with no consideration for input text. This means they carry no erroneous features that models can learn. By comparison, real label errors are almost always related to input text (Plank, Hovy, and Søgaard 2014).\nWe believe human-originated noising may enable future advancements across multiple areas of LNL, supporting new tasks and metrics in areas such as the cost of human reannotation, estimation of dataset error, and mitigation of bias.\nTakeaways: Be cautious when extrapolating from LNL findings which only used simple artificial noise. (Encourage creation of more challenging and realistic LNL benchmarks)\n\n\nEvaluating with noisy labels is as challenging as learning with noisy labels\nAs noise in evaluation splits can affect reported model performance as much as noise in training data, further study of model performance given noise in validation and test data seems essential.\nBoth clean and noisy performance on evaluation data provide useful insight into models’ overall performance. Very limited work done in this area: not possible to study noise in test before, because synthetic noise doesn’t confuse the model.\nTakeaway: (ENL)."
  },
  {
    "objectID": "blog.html#conclusions-and-future-work",
    "href": "blog.html#conclusions-and-future-work",
    "title": "Working with Label Errors in Natural Language Datasets",
    "section": "Conclusions and future work",
    "text": "Conclusions and future work\nFor practitioners: Data quality is important. Link to data-centric AI, etc.\nWe invite LNL researchers to try out our label noising library, and help to build a benchmark of realistic label errors. Our Mechanical Turk evaluation data is available here."
  },
  {
    "objectID": "blog.html#acknowledgments",
    "href": "blog.html#acknowledgments",
    "title": "Working with Label Errors in Natural Language Datasets",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nPlaceholder section"
  }
]